{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification and Analysis of Data Protection Acts\n",
    "\n",
    "This project focuses on the classification and analysis of data protection acts from Kenya, South Africa, and Europe. The goal is to build a robust text classification model using PyTorch and the Transformers library. The notebook is structured to guide you through the entire process, from data loading and preprocessing to model training and evaluation.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "### 1. Introduction\n",
    "- **Description**: Overview of the project, its objectives, and the datasets used.\n",
    "- **Libraries**: Import necessary libraries such as `torch`, `transformers`, `pandas`, and `sklearn`.\n",
    "\n",
    "### 2. Data Loading and Exploration\n",
    "- **Datasets Path**: Define the path to the datasets.\n",
    "- **Loading Data**: Load the GDPR dataset and display the first few rows.\n",
    "- **Data Cleaning**: Check for missing values and drop unnecessary columns.\n",
    "\n",
    "### 3. Data Preprocessing\n",
    "- **Text Preparation**: Combine relevant columns to create the input text for the model.\n",
    "- **Data Splitting**: Split the data into training, validation, and test sets.\n",
    "- **Text Cleaning**: Clean the input text by removing unwanted characters and spaces.\n",
    "\n",
    "### 4. Tokenization\n",
    "- **Tokenizer Initialization**: Initialize the BERT tokenizer.\n",
    "- **Tokenization Function**: Define a function to tokenize the data.\n",
    "- **Tokenize Data**: Tokenize the training, validation, and test datasets.\n",
    "\n",
    "### 5. Label Encoding\n",
    "- **Label Mapping**: Create a mapping of categories to numerical labels.\n",
    "- **Apply Labels**: Map the categories to labels in the datasets.\n",
    "- **Dataset Preparation**: Prepare the datasets for the model by creating a custom `Dataset` class.\n",
    "\n",
    "### 6. Model Training\n",
    "- **Model Initialization**: Initialize the BERT model for sequence classification.\n",
    "- **Training Arguments**: Define the training arguments such as batch size, learning rate, and number of epochs.\n",
    "- **Trainer Initialization**: Initialize the `Trainer` with the model, training arguments, and datasets.\n",
    "- **Model Training**: Train the model using the `Trainer`.\n",
    "\n",
    "### 7. Model Evaluation\n",
    "- **Evaluation**: Evaluate the model on the validation dataset.\n",
    "- **Metrics**: Display the evaluation metrics to assess the model's performance.\n",
    "\n",
    "### 8. Conclusion\n",
    "- **Summary**: Summarize the results and discuss potential improvements and future work.\n",
    "\n",
    "## Variables and Data Structures\n",
    "\n",
    "- **datasets_path**: Path to the datasets directory.\n",
    "- **gdpr**: DataFrame containing the GDPR dataset.\n",
    "- **gdpr_path**: Path to the GDPR CSV file.\n",
    "- **label_map**: Dictionary mapping categories to numerical labels.\n",
    "- **model**: BERT model for sequence classification.\n",
    "- **test_data**: DataFrame containing the test data.\n",
    "- **test_data_labels**: Numpy array of labels for the test data.\n",
    "- **test_dataset**: Custom dataset object for the test data.\n",
    "- **test_encoding**: Tokenized test data.\n",
    "- **tokenizer**: BERT tokenizer.\n",
    "- **train_data**: DataFrame containing the training data.\n",
    "- **train_dataset**: Custom dataset object for the training data.\n",
    "- **train_encoding**: Tokenized training data.\n",
    "- **trainer**: Trainer object for model training.\n",
    "- **training_args**: Training arguments for the Trainer.\n",
    "- **training_data**: DataFrame containing the original training data.\n",
    "- **training_data_copy**: Cleaned copy of the training data.\n",
    "- **training_labels**: Numpy array of labels for the training data.\n",
    "- **val_data**: DataFrame containing the validation data.\n",
    "- **val_dataset**: Custom dataset object for the validation data.\n",
    "- **val_encoding**: Tokenized validation data.\n",
    "- **val_labels**: Numpy array of labels for the validation data.\n",
    "\n",
    "This notebook provides a comprehensive guide to building a text classification model for data protection acts, leveraging the power of PyTorch and Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import transformers as tr\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Exploration\n",
    "\n",
    "#### Step 1: Importing Libraries\n",
    "- Import necessary libraries such as pandas, numpy, matplotlib, and seaborn for data manipulation and visualization.\n",
    "\n",
    "#### Step 2: Loading the Dataset\n",
    "- Setting the datasets path `../Datasets/` for all datasets for the project.\n",
    "- Load the dataset using pandas' `gdpr4.csv` function.\n",
    "- Display the first few rows of the dataset using the `head` method to get an initial understanding of the data structure.\n",
    "\n",
    "#### Step 3: Inspecting the Dataset\n",
    "- Use the `info` method to get a concise summary of the dataset, including the number of non-null entries and data types of each column.\n",
    "- Use the `describe` method to generate descriptive statistics that summarize the central tendency, dispersion, and shape of the datasetâ€™s distribution.\n",
    "\n",
    "#### Step 4: Checking for Missing Values\n",
    "- Check for missing values in the dataset using the `isnull` method combined with `sum` to get the total count of missing values per column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = '../Datasets/'\n",
    "gdpr_path = datasets_path + 'gdpr4.csv'\n",
    "gdpr = pd.read_csv(gdpr_path)\n",
    "gdpr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpr.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpr = gdpr.drop(columns=[\"Unnamed: 4\", \"Recitals\"], axis=1)\n",
    "gdpr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpr.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"input_text\"] = gdpr[\"Category\"] + \", \" + gdpr[\"Description\"] + \", \" + gdpr[\"GDPR Articles\"]\n",
    "train_data[\"input_text\"] = gdpr[\"Category\"] + \": \" + gdpr[\"Description\"] + \" (GDPR Article: \" + gdpr[\"GDPR Articles\"] + \")\"\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train_data[\"input_text\"] = train_data[\"input_text\"].str.replace(r\"\\(|\\)\", \"\", regex=True)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "train_data.to_csv('../Datasets/gdpr4_training_data.csv', index=False, quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract unique texts\n",
    "unique_text = train_data[\"input_text\"].str.split(\", \").str[0].unique()\n",
    "\n",
    "# Step 2: Create a label mapping\n",
    "label_mapping = {label: idx for idx, label in enumerate(unique_text)}\n",
    "print(label_mapping)\n",
    "\n",
    "# Step 3: Map the labels to integers\n",
    "train_data[\"label\"] = train_data[\"input_text\"].str.split(\", \").str[0].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "encoding = tokenizer(\n",
    "    train_data[\"input_text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, EvalPrediction\n",
    "\n",
    "training_data, test_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(training_data, test_size=0.1, random_state=42)\n",
    "\n",
    "def tokenize_data(df: pd.DataFrame, tokenizer: tr.PreTrainedTokenizer, max_length: int = 128) -> pd.DataFrame:\n",
    "    return tokenizer(\n",
    "        df[\"input_text\"].tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encoding = tokenize_data(train_data, tokenizer)\n",
    "val_encoding = tokenize_data(val_data, tokenizer)\n",
    "test_encoding = tokenize_data(test_data, tokenizer)\n",
    "print(train_encoding.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = [int(label) for label in labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = th.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "train_dataset = TextDataset(train_encoding, train_data[\"label\"].tolist())\n",
    "val_dataset = TextDataset(val_encoding, val_data[\"label\"].tolist())\n",
    "test_dataset = TextDataset(test_encoding, test_data[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"label\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = train_data[\"label\"].nunique()\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=28)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Directory to save model and logs\n",
    "    num_train_epochs=25,  # Number of training epochs\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation\n",
    "    warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    logging_steps=10,  # How often to log metrics\n",
    "    eval_strategy='steps',  # When to evaluate the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def train_model(learning_rate, batch_size):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=25,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size * 4,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        eval_strategy='steps',\n",
    "        learning_rate=learning_rate,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    return trainer.train()\n",
    "\n",
    "# Example usage of train_model\n",
    "best_accuracy = 0\n",
    "best_lr = None\n",
    "best_bs = None\n",
    "\n",
    "for lr in [1e-5, 3e-5, 5e-5]:\n",
    "    for bs in [8, 16, 32]:\n",
    "        train_output = train_model(lr, bs)\n",
    "        print(f'Available metrics: {train_output.metrics.keys()}')  # Debugging line\n",
    "        accuracy = train_output.metrics.get('eval_accuracy', 0)  # Use .get() to avoid KeyError\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_lr = lr\n",
    "            best_bs = bs\n",
    "\n",
    "print(f'Best Accuracy: {best_accuracy} with LR: {best_lr} and Batch Size: {best_bs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
